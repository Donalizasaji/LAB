{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHzEdGy1KTIrnDctH63jl2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Donalizasaji/LAB/blob/main/Dona_515_lab6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lrPgmQdB-Hx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Sample data: Replace with your parallel corpus (source, target pairs)\n",
        "source_texts = [\"hello\", \"how are you\", \"good morning\"]\n",
        "target_texts = [\"hola\", \"cómo estás\", \"buenos días\"]\n",
        "\n",
        "# Hyperparameters\n",
        "max_vocab_size = 10000\n",
        "max_len = 10\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "# Tokenization and padding\n",
        "source_tokenizer = Tokenizer(num_words=max_vocab_size, filters='')\n",
        "target_tokenizer = Tokenizer(num_words=max_vocab_size, filters='')\n",
        "\n",
        "source_tokenizer.fit_on_texts(source_texts)\n",
        "target_tokenizer.fit_on_texts(target_texts)\n",
        "\n",
        "source_sequences = source_tokenizer.texts_to_sequences(source_texts)\n",
        "target_sequences = target_tokenizer.texts_to_sequences(target_texts)\n",
        "\n",
        "source_padded = pad_sequences(source_sequences, maxlen=max_len, padding='post')\n",
        "target_padded = pad_sequences(target_sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "source_vocab_size = len(source_tokenizer.word_index) + 1\n",
        "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
        "\n",
        "# Transformer model\n",
        "class TransformerModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, units):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = LSTM(units, return_sequences=True, return_state=True)\n",
        "        self.dense = Dense(vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, encoder_input, decoder_input):\n",
        "        encoder_emb = self.embedding(encoder_input)\n",
        "        encoder_output, encoder_state_h, encoder_state_c = self.lstm(encoder_emb)\n",
        "\n",
        "        decoder_emb = self.embedding(decoder_input)\n",
        "        decoder_output, _, _ = self.lstm(decoder_emb, initial_state=[encoder_state_h, encoder_state_c])\n",
        "\n",
        "        output = self.dense(decoder_output)\n",
        "        return output\n",
        "\n",
        "# Instantiate model\n",
        "model = TransformerModel(vocab_size=source_vocab_size, embedding_dim=embedding_dim, units=units)\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model (input: encoder_input, decoder_input, target_output)\n",
        "encoder_input = source_padded\n",
        "decoder_input = np.zeros_like(target_padded)\n",
        "decoder_input[:, 1:] = target_padded[:, :-1]\n",
        "target_output = np.expand_dims(target_padded, -1)\n",
        "\n",
        "model.fit([encoder_input, decoder_input], target_output, batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "# Inference (translate new sentences)\n",
        "def translate(input_sentence):\n",
        "    input_seq = source_tokenizer.texts_to_sequences([input_sentence])\n",
        "    input_seq = pad_sequences(input_seq, maxlen=max_len, padding='post')\n",
        "\n",
        "    output_seq = np.zeros((1, max_len))\n",
        "    for i in range(max_len):\n",
        "        prediction = model.predict([input_seq, output_seq], verbose=0)\n",
        "        predicted_token = np.argmax(prediction[0, i, :])\n",
        "        output_seq[0, i] = predicted_token\n",
        "\n",
        "        if predicted_token == target_tokenizer.word_index['<end>']:  # Use <end> token to stop early\n",
        "            break\n",
        "\n",
        "    translated_sentence = ' '.join([target_tokenizer.index_word[int(token)] for token in output_seq[0] if token != 0])\n",
        "    return translated_sentence\n",
        "\n",
        "# Test translation\n",
        "translated_sentence = translate(\"hello\")\n",
        "print(f\"Translated: {translated_sentence}\")"
      ]
    }
  ]
}